# -*- coding: utf-8 -*-
"""news_cw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N3OXDtuQOGZcPY-lwG0Xs3gCmMruwcQI
"""

import os
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import FeatureUnion
from sklearn.feature_selection import VarianceThreshold
import re
import numpy as np

import operator
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from scipy.sparse import csr_matrix
from scipy.sparse import hstack

categories = ["business", "entertainment", "politics", "sport", "tech"]

current_directory = os.getcwd()
path = current_directory + "/bbc/"

def load_data(folder_path):
    documents = []
    labels = []
    for category in os.listdir(folder_path):
        count = 0
        category_path = os.path.join(folder_path, category)
        if os.path.isdir(category_path):
            for file_name in os.listdir(category_path):
                file_path = os.path.join(category_path, file_name)
                with open(file_path, 'r', encoding='latin1') as file:
                    content = file.read()
                    documents.append(content)
                    labels.append(category)
                    count += 1
        print(category, count)
    return documents, labels

def clean_text(text):
    text = re.sub(r'(\w+)-(\w+)', r'\1 \2', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\b\d+\b', '', text)
    text = ' '.join(text.split())

    return text

lemmatizer = nltk.stem.WordNetLemmatizer()

def get_list_tokens(string):
  sentence_split=nltk.tokenize.sent_tokenize(string)
  list_tokens=[]
  for sentence in sentence_split:
    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)
    for token in list_tokens_sentence:
      list_tokens.append(lemmatizer.lemmatize(token).lower())
  return list_tokens

def tokenize_words(texts, stop_words):
  token_list = []
  for i in texts:
    text_tokens = get_list_tokens(clean_text(i))
    filtered_text_tokens = [w for w in text_tokens if not w in stop_words]
    token_list.append(filtered_text_tokens)
  return token_list

def freq_dictionary(tokens):
  freq_dict = {}
  for i in tokens:
    for word in i:
      if word not in freq_dict:
        freq_dict[word] = 1
      else:
        freq_dict[word] += 1
  freq_dict_sorted = sorted(freq_dict.items(), key=operator.itemgetter(1), reverse=True)[:1000]

  return freq_dict_sorted

def get_vocabulary(tokens):
  vocabulary = []
  freq_dict = freq_dictionary(tokens)
  for word, frequency in freq_dict:
    vocabulary.append(word)

  return vocabulary

def get_vector_text(tokens, list_vocab):
  vector_text=np.zeros(len(list_vocab))
  for i, word in enumerate(list_vocab):
    if word in tokens:
      vector_text[i]=tokens.count(word)
  return vector_text

documents, labels = load_data(path)

X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)

token_list = []
stop_words = set(stopwords.words('english'))
token_list = tokenize_words(X_train, stop_words)
vocab = get_vocabulary(token_list)

X_train_bow, X_test_bow = [], []
for i in X_train:
  X_train_bow.append(get_vector_text(i, vocab))

for i in X_test:
  X_test_bow.append(get_vector_text(i, vocab))

#bi-gram
vectorizer = CountVectorizer(ngram_range=(2,2))
X_train_vectors = vectorizer.fit_transform(X_train)
X_test_vectors = vectorizer.transform(X_test)

# Create TF-IDF Vectors
tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 1))
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

X_train_bow = csr_matrix(np.array(X_train_bow))
X_test_bow = csr_matrix(np.array(X_test_bow))

#X_train_combined = np.concatenate((X_train_bow, X_train_vectors, X_train_tfidf), axis=1)
#X_test_combined = np.concatenate((X_test_bow, X_test_vectors, X_test_tfidf), axis=1)

print(X_train_bow.shape)
print(X_train_vectors.shape)
print(X_train_tfidf.shape)

X_train_combined = hstack([X_train_bow, X_train_vectors, X_train_tfidf])
X_test_combined = hstack([X_test_bow, X_test_vectors, X_test_tfidf])

threshold = 0.01
selector = VarianceThreshold(threshold)

X_train_filtered = selector.fit_transform(X_train_combined)
X_test_filtered = selector.transform(X_test_combined)

model = MultinomialNB()
model.fit(X_train_filtered, y_train)

y_pred = model.predict(X_test_filtered)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)